---
title: "High Performance Computing"
output: html_document
---

# High Performance Computing

Certain use cases require high performance computing resources:

  * big data
  * parallel computing
  * lengthy computation times
  * restricted-use data

For analyses involving big data or models that take a long time to estimate, a single laptop or desktop computer is often not powerful enough or becomes inconvenient to use. Additionally, for analyses involving restricted-use data, such as datasets containing personally identifiable information, data use agreements typically stipulate that the data should be stored and analyzed in a secure manner. In these cases you should use the high performance computing resources available to emLab, including cloud computing through Google Cloud Platform and the UCSB server clusters. Cloud computing incurs costs but is flexible whereas the UCSB server clusters are free but have some limitations.


## Google Cloud Platform




## UCSB Server Clusters

The Center for Scientific Computing at UCSB offers many resources for high performance computing to support research, including more than one server cluster for storing and analyzing data. Find out more information at <http://csc.cnsi.ucsb.edu/>. For a broader introduction to high performance computing with clusters see the HPC Carpentry's lessons at <https://hpc-carpentry.github.io/>.

### Accessing the server clusters

To access the server clusters, you must first request a user account. The request form can be found at <http://csc.cnsi.ucsb.edu/acct>. On the form, the job type selection should probably be GPU. In the job type description you should enter any relevant information related to user groups, access to specific folders, or special software usage. If needing Stata, you should request access under the `econ` group, which holds the license to use Stata. The system to be used should be Pod and Knot, which are the two primary Linux-based server clusters. Pod is the newest cluster and offers greater capabilities, but Knot is still adequate for most analysis needs and typically has less active users (i.e., a shorter queue). Either cluster allows 4 TB of storage per user and more can be requested if needed.

#### On campus

Once your account has been created, the clusters can be accessed on campus via a terminal emulator using the `ssh` command or via a GUI SSH app, such as [X2Go](https://wiki.x2go.org/doku.php). On macOS or Linux the default terminal apps can be used. On Windows the [PuTTY](https://www.putty.org/) or X2Go apps can be used. Once the terminal is started, to connect to the Knot or Pod clusters enter one of the commands

```
ssh user@knot.cnsi.ucsb.edu
# or
ssh user@pod.cnsi.ucsb.edu
```

where `user` is your server username. You will then be prompted for your password.

To use GUI applications on the clusters, which is not recommended because of performance issues, a slightly different process is needed in order to enter an XWindows environment. On macOS, [XQuartz](https://www.xquartz.org/) must be installed. On Windows, use the X2Go app to connect to the server. More info on X2Go can be found at <http://csc.cnsi.ucsb.edu/docs/using-x2go-gui-login-knot-or-pod>. To use GUIs, enter one of the commands

```
ssh -X user@knot.cnsi.ucsb.edu
# or
ssh -X user@pod.cnsi.ucsb.edu
```

Once connected, you will interact with the Linux server using the command line interface. The default working directory is your home folder `/home/user`. Note that this is the login node, but all analyses should be run on compute nodes. These analyses should be submitted as jobs (see below) to the queue, which allocates nodes.

To disconnect from the server, enter `exit`.

#### Off campus

The servers can be accessed off campus using the UCSB Pulse Secure Connect VPN service for secure remote access. Instructions for installing the VPN app can be found at <https://www.ets.ucsb.edu/services/campus-vpn/get-connected>. Once the app is installed, open it and select connect for UCSB Remote Access. You will then be prompted to enter your UCSBnetID and the associated password. Once connected, indicated by a green checkmark, you can then access the server as you normally would on campus by using a terminal or GUI SSH app.


### Managing files

File management on the server clusters involves commonly used Linux commands to navigate and modify files and folders. The generic setup is `command [options] arguments`. Most of the following commands have multiple options that are not covered here. For detailed documentation enter `man command`. Additionally, for a guided tutorial see the Software Carpentry's lesson on the Unix shell at <http://swcarpentry.github.io/shell-novice/>.

#### Navigation

`pwd` - present working directory

`cd dir` - change directory  
`cd` - change to home directory (`/home/user`)  
`cd ..` - change to directory above

`ls` - list all files in current directory  
`ls -a` - list all files, including hidden files  
`ls -l` - list all files with permission information

`cp source destination` - copy file  
`mv source destination` - move file

`mkdir name` - make directory  
`mkdir -m 770 name` - make directory with permissions (770 or other number)

`rm file` - remove file  
`rmdir dir` - remove empty directory (equivalent to `rm -d dir`)  
`rm -r dir` - remove non-empty directory

`shred -uz file` - securely overwrite and delete file

#### Viewing, creating, and editing files

`file file.txt` - view file type information  
`stat file.txt` - view file information

`more file.txt` - view text file one screen at a time (to exit press `q`)  
`less file.txt` - view text file with scrolling (to exit press `q`)

`head file.csv` - print first ten rows of text file  
`tail file.csv` - print last ten rows of text file

`head -1 file.csv` - print column names if in first row of text file

`cut -d , -f 2,4-6 file.csv | less` - view specific columns of .csv file by number position (e.g., 2,4-6)

To view .csv files with pretty output, enter  
`cat file.csv | sed -e 's/,,/, ,/g' | column -s, -t | less -#5 -N -S`  
Note: This may not work well for all files. Use arrow keys to navigate and enter `q` to exit.

`cat > file.txt` - create new text file  
`nano` - create new text file using nano text editor  
`nano file.txt` - view and edit text file using nano text editor

`zip -r file.zip file1 folder1` - create compressed .zip file with recursion  
`unzip file.zip` - extract .zip file into working directory

#### Permissions

Sometimes file and folder permissions need to be modified, such as to restrict access to files. On Linux, read, write, and execute permissions are represented by octal notation and applied to the file owner, groups, and all other users.

| # | Permission | rwx |
| :--- | :--- | :--- |
| 7 | read, write, and execute | rwx |
| 6 | read and write | rw- |
| 5 | read and execute | r-x |
| 4 | read only | r-- |
| 3 | write and execute | -wx |
| 2 | write only | -w- |
| 1 | execute only | --x |
| 0 | none | --- |

Common permissions include

`-rwxrwx---` = `770` - owner and group can do everything, but others can do nothing  
`-rwxr-x---` = `750` - owner can do everything, group can read and execute only, but others can do nothing  
`-rwx------` = `700` - owner can do everything, but group and others can do nothing

To change permissions enter

`chmod 770 dir` - change permissions for file or directory  
`chmod -R 770 dir` - change permissions recursively for directory  
`chgrp group file` - change group ownership for file or directory

#### Shortcuts

`Ctrl-A` - move to beginning of line  
`Ctrl-E` - move to end of line  
`Ctrl-U` - clear line from cursor  
`Ctrl-C` - cancel command  
`*` - wildcard completion  
`Tab key` - autocompletion  
`Up and down arrow keys` - cycle through command history

#### Transferring files

To transfer files to and from the server or your local computer, use `scp` or `rsync` commands in the terminal or use a SFTP GUI app. On macOS, [Cyberduck](https://cyberduck.io/) and [FileZilla](https://filezilla-project.org/) are good options. On Windows, [WinSCP](https://winscp.net/eng/index.php) and FileZilla are good options. On Linux, FileZilla is a good option. Another option is to use [Globus Online](http://csc.cnsi.ucsb.edu/docs/globus-online), a web app for transferring large files.


### Using rsync to transfer files

The rsync command line tool provides fast, incremental file transfer. It can be used to transfer files between servers or between a local computer and a server. The primary use case is to sync two folders, such as a synced backup folder. For more detailed documentation see <https://rsync.samba.org/>.

The generic setup for an rsync command is

```
rsync [options] source destination
```

The typical options used, here with an example of transferring an entire directory from a local machine to the Pod server cluster, are

```
rsync -avzP -e ssh /home/user/project/ user@pod.cnsi.ucsb.edu:/home/user/project
```

This example represents a push transfer. A pull transfer could also be completed by switching the source and destination. When transferring entire directories, a forward slash on the source matters but never for the destination. In this case, the 'project' folder has a forward slash and all its contents will be replicated exactly in the destination 'project' folder, copying all contents except the top 'project' folder. Without the forward slash on the source there would be another 'project' folder within the 'project' folder at the destination.

The rsync options can be specified in short or long forms. In this use case, the `-a`  or `--archive` option completely replicates all folders and files, including recursively through all subdirectories while preserving symbolic links, permissions, and ownership. The `-v` or `--verbose` option increases the amount of information that is logged. The `-z` or `--compress` option compresses files during transit to reduce transfer time. The `-P` or `--partial --progress` option enables partially transferred files to be kept in case of a break or pause and also displays the progress of individual file transfers. Many other options exist, but these are the primary ones. Additionally, add the `-n` or `--dry-run` option to test what an rsync command will do without making changes.

After the command is submitted, the user will be prompted for a password if transferring to or from a server. If using rsync frequently, SSH keys can also be set up to skip password entry.

For large transfers, sometimes the process needs to be paused and resumed, or sometimes the transfer can be interrupted due to a server, network, or power outage. The `-P` or `--partial` option should always be used to preserve the files or parts of files that have already been transferred and to avoid having to start over. To pause a transfer, use `Ctrl-C`. To resume, resubmit the same rsync command with the `--append` option added, which will restart the transfer where it left off. When syncing to a backup folder, add the `--delete` option to delete files and folders that have been deleted in the source.


### Submitting jobs

The server clusters are shared resources, and a job queue process is used to manage and allocate resources among users. A job is simply a set of instructions that includes requests for resources and the specific set of commands to be executed, such as commands for analyzing data. When a user submits a job to the server for execution it enters the queue and is scheduled for a specific time.

#### Knot

The Knot cluster uses [TORQUE PBS](http://csc.cnsi.ucsb.edu/docs/example-scripts-running-jobs) to schedule jobs.

First create a new .pbs file with the command

```
nano submit.pbs
```

The typical structure of a .pbs file for a serial job using R is

```
#!/bin/bash
#PBS -l nodes=1:ppn=12
#PBS -l walltime=2:00:00
#PBS -V

cd $PBS_O_WORKDIR

Rscript script.R
```

where 1 node is requested with a 2-hour timeframe for computation. The `walltime` option can be excluded if the computation time is unknown. The `Rscript script.R` line executes the commands in the specified R script. The filepath for the script should be relative to where the .pbs file is located or an absolute path can also be used. This line would change if using different software. There are also many other `#PBS` options that can be included. Parallel jobs require different specifications, such as requesting more than 1 node and using `mpirun` commands.

For another example, the structure for a serial job using Stata and that sends e-mail notifications when a job begins and ends is

```
#!/bin/bash
#PBS -l nodes=1:ppn=12
#PBS -V
#PBS -m be
#PBS -M user@ucsb.edu

cd $PBS_O_WORKDIR/

/sw/stata/stata-mp -b do script.do
```

To submit a job, use the command

```
qsub submit.pbs
```

For jobs that require less than one hour, use the short queue to minimize waiting time with the command

```
qsub -q short submit.pbs
```

For jobs that require large memory, use one of the commands 

```
# 256 GB/node
qsub -q largemem submit.pbs 
# or 512 GB/node
qsub -q xlargemem submit.pbs
```

The `qsub` command will return a job number. To check the status of a job, use the command

```
qstat [job number]
```

To cancel or delete a job, use the command

```
qdel [job number]
```

The outputs of the analysis in the script will be returned in the same folder as the .pbs file that was submitted, typically with the filename structure `submit.pbs.[job number]` unless otherwise specified.

#### Pod

The Pod cluster uses [SLURM](http://csc.cnsi.ucsb.edu/docs/slurm-job-scheduler) to schedule jobs.

First create a new .job file with the command

```
nano submit.job
```

The typical structure of a .job file for a serial job using R is

```
#!/bin/bash -l
#Serial
#SBATCH --nodes=1 --ntasks-per-node=1

cd $SLURM_SUBMIT_DIR

module load R
Rscript script.R
```

where 1 node is requested. The `module load R` line loads R, and then the `Rscript script.R` line executes the commands in the specified R script. The filepath for the script should be relative to where the .job file is located or an absolute path can also be used. These last two lines would change if using different software. There are also many other `#SBATCH` options that can be included. Parallel jobs require different specifications, such as requesting more than 1 node and using `mpirun` commands.

For another example, the structure for a serial job using Stata and that sends e-mail notifications is

```
#!/bin/bash -l
#Serial
#SBATCH --nodes=1 --ntasks-per-node=1
#SBATCH --mail-type=ALL
#SBATCH --mail-user=user@ucsb.edu

cd $SLURM_SUBMIT_DIR

/sw/stata/stata-mp -b do script.do
```

To submit a job, use the command

```
sbatch submit.job
```

For jobs that require less than one hour, use the short queue to minimize waiting time with the command

```
sbatch -p short submit.job
```

For jobs that require large memory, use the command

```
# 1.5 TB/node
sbatch -p fat submit.job
```

The `sbatch` command will return a job number. To check the status of a job, use one of the commands

```
showq [job number]
# or
showq | grep mdono
# or
squeue -u $USER
```

To cancel or delete a job, use the command

```
scancel -i [job number]
```

The outputs of the analysis in the script will be returned in the same folder as the .job file that was submitted, typically with the filename structure `slurm-[jobnumber].out` unless otherwise specified.


#### Available software

The software available on the clusters includes R, Stata, Python, Julia, MATLAB, Git, and LaTeX. Additional information about available software, using different versions of software, and installing packages can be found at <http://csc.cnsi.ucsb.edu/docs/scientific-software>.
