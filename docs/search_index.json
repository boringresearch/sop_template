[
["index.html", "emLab Standard Operating Procedures Overview", " emLab Standard Operating Procedures 2019-05-23 Overview This reference guide describes standard operating procedures for emLab projects. "],
["1-project-management.html", "1 Project Management ", " 1 Project Management "],
["1-1-airtable.html", "1.1 Airtable", " 1.1 Airtable "],
["1-2-google-team-drive.html", "1.2 Google Team Drive", " 1.2 Google Team Drive "],
["1-3-github.html", "1.3 GitHub", " 1.3 GitHub "],
["2-data.html", "2 Data ", " 2 Data "],
["2-1-tidy-data.html", "2.1 Tidy Data", " 2.1 Tidy Data As researchers, we work with data from many different sources. Often these data are messy. One of the first steps of any analysis is to clean any available raw data so that you can make simple visualizations of the data, calculate simple summary statistics, look for missing or incorrect data, and eventually proceed with more involved analyses or modeling. As part of the data cleaning process, we recommend getting all data into a “tidy” format (also known as “long” format, as opposed to “wide” format). According to the Tidy Data Guide by Garrett Grolemund and Hadley Wickham, tidy data is defined as: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. These three features of tidy data can be seen in the following figure, also from the Tidy Data Guide: Once data are in this format, it makes subsequent visualization and analysis much easier. If you’ve ever worked with a file that has a separate column for each year (an example of “wide” format data), you know how hard that type of data format is to work with! As always, we recommend keeping a backup copy of the raw data you obtained from the original source, and using a reproducible script for transforming these data into a tidy data format. 2.1.1 Recommended Resources We highly recommend the chapter on Tidy Data from the book R for Data Science by Garrett Grolemund and Hadley Wickham. This guide is geared towards R users and provides helpful tips for transforming and working with data in R, but the concepts should be broadly applicable to other languages as well. For tips specific to Python, we recommend a blogpost by Jean-Nicholas Hould titled Tidy Data in Python. "],
["2-2-storage.html", "2.2 Storage", " 2.2 Storage "],
["2-3-metadata-and-data-directory.html", "2.3 Metadata and Data Directory", " 2.3 Metadata and Data Directory "],
["3-code.html", "3 Code ", " 3 Code "],
["3-1-scripts-and-version-control.html", "3.1 Scripts and Version Control", " 3.1 Scripts and Version Control "],
["3-2-style-guide.html", "3.2 Style Guide", " 3.2 Style Guide At emLab, we recommend using a consistent code style for each of the different programming languages we use. Recommended coding styles for a particular language are collated in what is known as a “style guide”. Style guides typically include standardized ways of naming script files, defining functions and variables, commenting code, etc. While emLab does not mandate the use of style guides, having a consistent code style allows all emLab staff to easily understand each other’s code, collaborate on projects, and jump in on new projects. We consider this an important aspect of collaboration, reproducibility, and transparency. Rather than re-invent the wheel, we leverage existing code style guidelines for each of the languages we use. Below is a summary of the code style guidelines we recommend for various languages: R - Tidyverse Style Guide, by Hadley Wickham Python - Google’s Python Style Guide Stata - Suggestions on Stata programming style, by Nicolas J. Cox Other languages - Google style guides for other languages "],
["3-3-github-workflow.html", "3.3 GitHub Workflow", " 3.3 GitHub Workflow "],
["4-high-performance-computing.html", "4 High Performance Computing", " 4 High Performance Computing Sometimes a single laptop or desktop computer is not powerful enough for your data analysis needs. In these cases it is advisable to use high performance computing resources such as cloud computing or the UCSB server clusters. "],
["4-1-google-cloud-platform.html", "4.1 Google Cloud Platform", " 4.1 Google Cloud Platform "],
["4-2-ucsb-server-clusters.html", "4.2 UCSB Server Clusters", " 4.2 UCSB Server Clusters The Center for Scientific Computing at UCSB offers many resources for high performance computing to support research, including more than one server cluster for storing and analyzing data. Find out more information at http://csc.cnsi.ucsb.edu/. For a broader introduction to high performance computing with clusters see the HPC Carpentry’s lessons at https://hpc-carpentry.github.io/. 4.2.1 Accessing the server clusters To access the server clusters, you must first request a user account. The request form can be found at http://csc.cnsi.ucsb.edu/acct. On the form, the job type selection should probably be GPU. In the job type description you should enter any relevant information related to user groups, access to specific folders, or special software usage. If needing Stata, you should request access under the econ group, which holds the license to use Stata. The system to be used should be Pod and Knot, which are the two primary Linux-based server clusters. Pod is the newest cluster and offers greater capabilities, but Knot is still adequate for most analysis needs and typically has less active users (i.e., a shorter queue). Either cluster allows 4 TB of storage per user and more can be requested if needed. 4.2.1.1 On campus Once your account has been created, the clusters can be accessed on campus via a terminal emulator using the ssh command or via a GUI SSH app, such as X2Go. On macOS or Linux the default terminal apps can be used. On Windows the PuTTY or X2Go apps can be used. Once the terminal is started, to connect to the Knot or Pod clusters enter one of the commands ssh user@knot.cnsi.ucsb.edu # or ssh user@pod.cnsi.ucsb.edu where user is your server username. You will then be prompted for your password. To use GUI applications on the clusters, which is not recommended because of performance issues, a slightly different process is needed in order to enter an XWindows environment. On macOS, XQuartz must be installed. On Windows, use the X2Go app to connect to the server. More info on X2Go can be found at http://csc.cnsi.ucsb.edu/docs/using-x2go-gui-login-knot-or-pod. To use GUIs, enter one of the commands ssh -X user@knot.cnsi.ucsb.edu # or ssh -X user@pod.cnsi.ucsb.edu Once connected, you will interact with the Linux server using the command line interface. The default working directory is your home folder /home/user. Note that this is the login node, but all analyses should be run on compute nodes. These analyses should be submitted as jobs (see below) to the queue, which allocates nodes. To disconnect from the server, enter exit. 4.2.1.2 Off campus The servers can be accessed off campus using the UCSB Pulse Secure Connect VPN service for secure remote access. Instructions for installing the VPN app can be found at https://www.ets.ucsb.edu/services/campus-vpn/get-connected. Once the app is installed, open it and select connect for UCSB Remote Access. You will then be prompted to enter your UCSBnetID and the associated password. Once connected, indicated by a green checkmark, you can then access the server as you normally would on campus by using a terminal or GUI SSH app. 4.2.2 Managing files File management on the server clusters involves commonly used Linux commands to navigate and modify files and folders. The generic setup is command [options] arguments. Most of the following commands have multiple options that are not covered here. For detailed documentation enter man command. Additionally, for a guided tutorial see the Software Carpentry’s lesson on the Unix shell at http://swcarpentry.github.io/shell-novice/. 4.2.2.1 Navigation pwd - present working directory cd dir - change directory cd - change to home directory (/home/user) cd .. - change to directory above ls - list all files in current directory ls -a - list all files, including hidden files ls -l - list all files with permission information cp source destination - copy file mv source destination - move file mkdir name - make directory mkdir -m 770 name - make directory with permissions (770 or other number) rm file - remove file rmdir dir - remove empty directory (equivalent to rm -d dir) rm -r dir - remove non-empty directory shred -uz file - securely overwrite and delete file 4.2.2.2 Viewing, creating, and editing files file file.txt - view file type information stat file.txt - view file information more file.txt - view text file one screen at a time (to exit press q) less file.txt - view text file with scrolling (to exit press q) head file.csv - print first ten rows of text file tail file.csv - print last ten rows of text file head -1 file.csv - print column names if in first row of text file cut -d , -f 2,4-6 file.csv | less - view specific columns of .csv file by number position (e.g., 2,4-6) To view .csv files with pretty output, enter cat file.csv | sed -e 's/,,/, ,/g' | column -s, -t | less -#5 -N -S Note: This may not work well for all files. Use arrow keys to navigate and enter q to exit. cat &gt; file.txt - create new text file nano - create new text file using nano text editor nano file.txt - view and edit text file using nano text editor zip -r file.zip file1 folder1 - create compressed .zip file with recursion unzip file.zip - extract .zip file into working directory 4.2.2.3 Permissions Sometimes file and folder permissions need to be modified, such as to restrict access to files. On Linux, read, write, and execute permissions are represented by octal notation and applied to the file owner, groups, and all other users. # Permission rwx 7 read, write, and execute rwx 6 read and write rw- 5 read and execute r-x 4 read only r– 3 write and execute -wx 2 write only -w- 1 execute only –x 0 none — Common permissions include -rwxrwx--- = 770 - owner and group can do everything, but others can do nothing -rwxr-x--- = 750 - owner can do everything, group can read and execute only, but others can do nothing -rwx------ = 700 - owner can do everything, but group and others can do nothing To change permissions enter chmod 770 dir - change permissions for file or directory chmod -R 770 dir - change permissions recursively for directory chgrp group file - change group ownership for file or directory 4.2.2.4 Shortcuts Ctrl-A - move to beginning of line Ctrl-E - move to end of line Ctrl-U - clear line from cursor Ctrl-C - cancel command * - wildcard completion Tab key - autocompletion Up and down arrow keys - cycle through command history 4.2.2.5 Transferring files To transfer files to and from the server or your local computer, use scp or rsync commands in the terminal or use a SFTP GUI app. On macOS, Cyberduck and FileZilla are good options. On Windows, WinSCP and FileZilla are good options. On Linux, FileZilla is a good option. Another option is to use Globus Online, a web app for transferring large files. 4.2.3 Using rsync to transfer files The rsync command line tool provides fast, incremental file transfer. It can be used to transfer files between servers or between a local computer and a server. The primary use case is to sync two folders, such as a synced backup folder. For more detailed documentation see https://rsync.samba.org/. The generic setup for an rsync command is rsync [options] source destination The typical options used, here with an example of transferring an entire directory from a local machine to the Pod server cluster, are rsync -avzP -e ssh /home/user/project/ user@pod.cnsi.ucsb.edu:/home/user/project This example represents a push transfer. A pull transfer could also be completed by switching the source and destination. When transferring entire directories, a forward slash on the source matters but never for the destination. In this case, the ‘project’ folder has a forward slash and all its contents will be replicated exactly in the destination ‘project’ folder, copying all contents except the top ‘project’ folder. Without the forward slash on the source there would be another ‘project’ folder within the ‘project’ folder at the destination. The rsync options can be specified in short or long forms. In this use case, the -a or --archive option completely replicates all folders and files, including recursively through all subdirectories while preserving symbolic links, permissions, and ownership. The -v or --verbose option increases the amount of information that is logged. The -z or --compress option compresses files during transit to reduce transfer time. The -P or --partial --progress option enables partially transferred files to be kept in case of a break or pause and also displays the progress of individual file transfers. Many other options exist, but these are the primary ones. Additionally, add the -n or --dry-run option to test what an rsync command will do without making changes. After the command is submitted, the user will be prompted for a password if transferring to or from a server. If using rsync frequently, SSH keys can also be set up to skip password entry. For large transfers, sometimes the process needs to be paused and resumed, or sometimes the transfer can be interrupted due to a server, network, or power outage. The -P or --partial option should always be used to preserve the files or parts of files that have already been transferred and to avoid having to start over. To pause a transfer, use Ctrl-C. To resume, resubmit the same rsync command with the --append option added, which will restart the transfer where it left off. When syncing to a backup folder, add the --delete option to delete files and folders that have been deleted in the source. 4.2.4 Submitting jobs The server clusters are shared resources, and a job queue process is used to manage and allocate resources among users. A job is simply a set of instructions that includes requests for resources and the specific set of commands to be executed, such as commands for analyzing data. When a user submits a job to the server for execution it enters the queue and is scheduled for a specific time. 4.2.4.1 Knot The Knot cluster uses TORQUE PBS to schedule jobs. First create a new .pbs file with the command nano submit.pbs The typical structure of a .pbs file for a serial job using R is #!/bin/bash #PBS -l nodes=1:ppn=12 #PBS -l walltime=2:00:00 #PBS -V cd $PBS_O_WORKDIR Rscript script.R where 1 node is requested with a 2-hour timeframe for computation. The walltime option can be excluded if the computation time is unknown. The Rscript script.R line executes the commands in the specified R script. The filepath for the script should be relative to where the .pbs file is located or an absolute path can also be used. This line would change if using different software. There are also many other #PBS options that can be included. Parallel jobs require different specifications, such as requesting more than 1 node and using mpirun commands. For another example, the structure for a serial job using Stata and that sends e-mail notifications when a job begins and ends is #!/bin/bash #PBS -l nodes=1:ppn=12 #PBS -V #PBS -m be #PBS -M user@ucsb.edu cd $PBS_O_WORKDIR/ /sw/stata/stata-mp -b do script.do To submit a job, use the command qsub submit.pbs For jobs that require less than one hour, use the short queue to minimize waiting time with the command qsub -q short submit.pbs For jobs that require large memory, use one of the commands # 256 GB/node qsub -q largemem submit.pbs # or 512 GB/node qsub -q xlargemem submit.pbs The qsub command will return a job number. To check the status of a job, use the command qstat [job number] To cancel or delete a job, use the command qdel [job number] The outputs of the analysis in the script will be returned in the same folder as the .pbs file that was submitted, typically with the filename structure submit.pbs.[job number] unless otherwise specified. 4.2.4.2 Pod The Pod cluster uses SLURM to schedule jobs. First create a new .job file with the command nano submit.job The typical structure of a .job file for a serial job using R is #!/bin/bash -l #Serial #SBATCH --nodes=1 --ntasks-per-node=1 cd $SLURM_SUBMIT_DIR module load R Rscript script.R where 1 node is requested. The module load R line loads R, and then the Rscript script.R line executes the commands in the specified R script. The filepath for the script should be relative to where the .job file is located or an absolute path can also be used. These last two lines would change if using different software. There are also many other #SBATCH options that can be included. Parallel jobs require different specifications, such as requesting more than 1 node and using mpirun commands. For another example, the structure for a serial job using Stata and that sends e-mail notifications is #!/bin/bash -l #Serial #SBATCH --nodes=1 --ntasks-per-node=1 #SBATCH --mail-type=ALL #SBATCH --mail-user=user@ucsb.edu cd $SLURM_SUBMIT_DIR /sw/stata/stata-mp -b do script.do To submit a job, use the command sbatch submit.job For jobs that require less than one hour, use the short queue to minimize waiting time with the command sbatch -p short submit.job For jobs that require large memory, use the command # 1.5 TB/node sbatch -p fat submit.job The sbatch command will return a job number. To check the status of a job, use one of the commands showq [job number] # or showq | grep mdono # or squeue -u $USER To cancel or delete a job, use the command scancel -i [job number] The outputs of the analysis in the script will be returned in the same folder as the .job file that was submitted, typically with the filename structure slurm-[jobnumber].out unless otherwise specified. 4.2.4.3 Available software The software available on the clusters includes R, Stata, Python, Julia, MATLAB, Git, and LaTeX. Additional information about available software, using different versions of software, and installing packages can be found at http://csc.cnsi.ucsb.edu/docs/scientific-software. "]
]
