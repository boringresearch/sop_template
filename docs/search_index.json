[
["index.html", "emLab Standard Operating Procedures Overview", " emLab Standard Operating Procedures 2019-08-29 Overview This reference guide describes standard operating procedures for emLab projects. "],
["1-project-management.html", "1 Project Management", " 1 Project Management This section details the project management platforms we use at emLab to help set up projects for success. Please additionally review the project management best practices document. "],
["1-1-google-calendar.html", "1.1 Google Calendar", " 1.1 Google Calendar Our team relies heavily on Google Calendar to check team member’s availability and schedule meetings. Please keep your calendar up to date! 1.1.1 Setting out of office notifications The best way to let people know if you are on vacation or out of the office is by setting up your calendar event as “Out of Office” instead of a regular event. You can customize this to automatically decline meetings on that day. 1.1.2 Adding other calendars To view team member’s calendars, click on the plus symbol next to “Other calendars” on the left hand side of your calendar and select “Subscribe to calendar.” There is a full list of team emails in the emLab Team Roster document. 1.1.3 Scheduling rooms See this Room Scheduling document for full details on how to reserve rooms in both Bren and MSI. As a reminder, please add bren-sfg@ucsb.edu to any meeting invite you want hold in MSI 1304. "],
["1-2-slack.html", "1.2 Slack", " 1.2 Slack Our team uses Slack on a daily basis to keep communication channels open within teams and across our different offices. We have an emLab workspace that houses all of our channels. When you are first added to Slack, you will be added to the General, Random, Communications, Report and Publications, and Code channels. We also have a channel for every project, which you will be aded to as needed. We recommend downloading the desktop app, but you can also view Slack in a browser window. 1.2.1 Slack basics Slack is organized into Channels and Direct Messages. Channels are a way to organize conversations and other than a couple general emLab channels, are often project specific. Whatever you share in a channel is viewable by all members of that channel. You can also send direct messages to an individual or a group of up to 9 people. One great thing about Slack is that it’s searchable. You can search either by person or keywords to find old messages. Additionally, if someone sends you something you will need to reference multiple times, you can star messages and view them by clicking the star in the upper righthand corner. People have differing notification preferences, which you can set under Preferences → Notifications. If someone sends you a direct message or tags you, a number will show up on your slack app. If they add something to a channel you are on but don’t tag you, a red dot will show up. To ensure someone gets a numbered notification (number is based on the number of messages), either tag them (i.e. @Erin) or tag the channel (i.e. @channel). Tagging the channel will send a notification to every member of that channel. 1.2.2 Creating a channel When a new project starts, create a slack channel for it and add the relevant team members to it. To create a channel, simply click on the plus symbol next to channels and fill out the channel information (Name, Purpose, and Send invites to). It’s that easy! "],
["1-3-airtable.html", "1.3 Airtable", " 1.3 Airtable At emLab, we use Airtable as our primary project management software. To put it simply, it’s a glorified spreadsheet that you can mold to fit your needs. Email Erin (eoreilly@ucsb.edu) to be added to Airtable. 1.3.1 Helpful terminology Base: think of it like a database. A base is made up of tabs of spreadsheets that can be customized and linked to one another Workspace: multiple bases can be organized within a workspace. For example, we have an emLab workspace and individual project bases. 1.3.2 Getting to know the emLab workspace Within the emLab workspace, we have the following bases: emLab projects and pipeline Project template Project specific The emLab Projects and Pipeline base contains overview information about current and potential projects. It is organized into the following tabs: Team Directory, Projects (current and archived), Deliverables (as outlined in the scope of work), and In the Pipeline (potential projects and their current stage). The Project Template provides the basic structure for a project specific base. To copy, click on the arrow in the bottom right hand corner and select “duplicate base.” Then rename the base and customize it to the needs of your team. Each emLab project has its own base. Teams can customize these as they see fit, but at a minimum, please include the deliverables in the project’s scope of work and the team. Within the team tab, it is often helpful to add everyone’s project role because these might differ from project to project (i.e. might be a project manager on one and a researcher on another). 1.3.3 Customizing your project base It is up to you and your team to determine how to make Airtable work for you. Outlined below are different ways to customize your base. Feel free to look at other bases that have been created for inspiration! 1.3.3.1 Tab ideas Deliverables (required): high level deliverables stated in the scope of work Tasks: more detailed steps of how to reach a deliverable; could be day-to-day tasks Note: you can link a tasks tab with the deliverables tab to see how they feed into one another Team (required): list of team members with their emails and project role Note: this can be linked to the deliverables and tasks tab so you know who is working on what Datasets: way to keep track of all the datasets going into an analysis and what stage of collection and processing they are in Example: for the NatGeo Conservation Priorities project, they grouped data by goal (biodiversity, food provision, impacts, carbon), whether it had been collected, the stage of processing, data source, and description of the data Analyses: tracking different versions of a model run to keep track of progress and outputs of different simulations run Example: for the NatGeo Conservation Priorities project, they listed each version and what was run in that version, and attached figures of the results 1.3.3.2 Column options There are 25 column type options within Airtable. The ones we most commonly use are: Link to another record: links information from different tabs within the same base Example: create an “assigned to” column in your deliverables tab and link it with your team tab. When looking at the deliverables tab, you can see how tasks are distributed across the team, and when looking at the team tab, you can see what deliverables each person is working on. Single line text Attachment Checkbox Multiple select Single select Date 1.3.3.3 Ways to view your base Different views There are 5 different ways to view your base: Grid: view as a spreadsheet (most used!) Kanaban: view in columns (similar to Trello) Calendar: view items on a calendar Gallery: view each entry as its own card Form: creates something similar to a GoogleForm You can also name different views of the same tab. For example, say you have a tasks tab and want to separately group them by deliverable and view what items are complete. Instead of manually changing this each time, create and name one view that groups by deliverable and then create and name another view that groups by completed. This way you can toggle back and forth with ease. Filtering This works like the normal filtering function in spreadsheets. You can sort A to Z, by date, exclude records with certain names, etc. Grouping Grouping allows you to bucket your spreadsheet by field type. Example 1: Say you have a “Complete?” checkbox column. If you choose to group by “Complete?” it will place all records with the box checked in one bucket and all records that aren’t checked in another bucket. Example 2: For the clean seafood project, there are four different research tracks and general items for the team to complete. If you create a “Research Track” column with a dropdown of the different tracks and group by track, you can view deliverables separated out by research track. Expanding record Within each tab, you can expand records in the first column to view all its information at once by clicking the two opposing arrows before the text. This option combines information from all of the columns into an easy to read card format. "],
["1-4-google-shared-drive.html", "1.4 Google Shared Drive", " 1.4 Google Shared Drive We have all experienced the moment where we can’t remember where a GoogleDoc is. To help solve this problem, we created an emLab Shared Google Drive, which is a centralized space for all of our documents to live within a shared file structure. Unlike files in My Drive (your personal Google Drive account), files within the Shared Drive belong to the team instead of an individual. So even if people leave, the files stay exactly where they are and aren’t lost with that person’s account. You can read more about Shared Drives here. 1.4.1 General structure The emLab Shared Drive is organized into four main folders: Central emLab Resources: includes meeting and event information, onboarding materials, information about travel reimbursements, and the team roster Communications: includes the blog schedule, Adobe design projects, powerpoint templates, photo repository, and publication and media tracking Data: will include the emLab data directory and all datasets we work with Projects: includes information on past and current projects, and project management guidelines A full table of contents can be seen here. Each project folder must contain the following 5 folders: Deliverables Grant Reporting Meetings and Events Presentations Project Materials From here, each project can add sub-folders as they see fit. 1.4.2 Sharing files Note: for Shared Drive, you either add someone to the entire Drive or share individual files with them. You cannot share individual folders within a Shared Drive. Members of the Shared Drive can see all folders and files within the Drive. If you want to send a quick link to someone who is part of the Shared Drive, simply copy the URL from your browser. There is no need to create a shared link if you are sending it to someone with access to the emLab Shared Drive. If you want to share files with people outside of the emLab Shared Drive, that is still possible. Within a document, click on Share in the upper right hand corner. From there are two options: either add people’s emails or create a shared link. To create a shared link, click Who Has Access and turn link sharing on. From there you can select the level of editing capabilities and either keep the just UCSB access or expand to allow external access. 1.4.3 How to install locally (Juan Carlos) Background of why people would want to install drive file stream Include both simple instructions and instructions of how to deal with mounting 2 different accounts "],
["1-5-github.html", "1.5 GitHub", " 1.5 GitHub "],
["2-data.html", "2 Data", " 2 Data Data should be managed and shared properly to make it useful in research, to promote transparency and facilitate reproducibility, and to ensure the credibility of research. This includes such practices as transforming data into a tidy format, storing data in open file formats, and providing data documentation. Recommended readings: Borer, Elizabeth T., Eric W. Seabloom, Matthew B. Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” The Bulletin of the Ecological Society of America 90 (2): 205–14. https://doi.org/10.1890/0012-9623-90.2.205. Broman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989. Ellis, Shannon E., and Jeffrey T. Leek. 2018. “How to Share Data for Collaboration.” The American Statistician 72 (1): 53–57. https://doi.org/10.1080/00031305.2017.1375987. Goodman, Alyssa, Alberto Pepe, Alexander W. Blocker, Christine L. Borgman, Kyle Cranmer, Merce Crosas, Rosanne Di Stefano, et al. 2014. “Ten Simple Rules for the Care and Feeding of Scientific Data.” PLOS Computational Biology 10 (4): e1003542. https://doi.org/10.1371/journal.pcbi.1003542. Hart, Edmund M., Pauline Barmby, David LeBauer, François Michonneau, Sarah Mount, Patrick Mulrooney, Timothée Poisot, Kara H. Woo, Naupaka B. Zimmerman, and Jeffrey W. Hollister. 2016. “Ten Simple Rules for Digital Data Storage.” PLOS Computational Biology 12 (10): e1005097. https://doi.org/10.1371/journal.pcbi.1005097. Wilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (March): 160018. https://doi.org/10.1038/sdata.2016.18. "],
["2-1-tidy-data.html", "2.1 Tidy Data", " 2.1 Tidy Data As researchers, we work with data from many different sources. Often these data are messy. One of the first steps of any analysis is to clean any available raw data so that you can make simple visualizations of the data, calculate simple summary statistics, look for missing or incorrect data, and eventually proceed with more involved analyses or modeling. As part of the data cleaning process, we recommend getting all data into a “tidy” format (also known as “long” format, as opposed to “wide” format). According to the Tidy Data Guide by Garrett Grolemund and Hadley Wickham, tidy data is defined as: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. These three features of tidy data can be seen in the following figure, also from the Tidy Data Guide: Once data are in this format, it makes subsequent visualization and analysis much easier. If you’ve ever worked with a file that has a separate column for each year (an example of “wide” format data), you know how hard that type of data format is to work with! As always, we recommend keeping a backup copy of the raw data you obtained from the original source, and using a reproducible script for transforming these data into a tidy data format. 2.1.1 Recommended Resources We highly recommend the chapter on Tidy Data from the book R for Data Science by Garrett Grolemund and Hadley Wickham. This guide is geared towards R users and provides helpful tips for transforming and working with data in R, but the concepts should be broadly applicable to other languages as well. For tips specific to Python, we recommend a blogpost by Jean-Nicholas Hould titled Tidy Data in Python. "],
["2-2-storage.html", "2.2 Storage", " 2.2 Storage "],
["2-3-metadata-and-data-directory.html", "2.3 Metadata and Data Directory", " 2.3 Metadata and Data Directory "],
["3-code.html", "3 Code", " 3 Code All data transformation and analysis should be performed with code (i.e., avoid spreadsheets), and all code should be packaged in scripts that are version controlled and follow a style guide. Using code and scripts allows for better organization, documentation, and reproducibility of analysis workflows. Recommended readings: Bryan, Jennifer. 2018. “Excuse Me, Do You Have a Moment to Talk About Version Control?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928. Stodden, Victoria, and Sheila Miguez. 2014. “Best Practices for Computational Science: Software Infrastructure and Environments for Reproducible and Extensible Research.” Journal of Open Research Software 2 (1): e21. https://doi.org/10.5334/jors.ay. Wilson, Greg, D. A. Aruliah, C. Titus Brown, Neil P. Chue Hong, Matt Davis, Richard T. Guy, Steven H. D. Haddock, et al. 2014. “Best Practices for Scientific Computing.” PLOS Biology 12 (1): e1001745. https://doi.org/10.1371/journal.pbio.1001745. Wilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510. "],
["3-1-scripts-and-version-control.html", "3.1 Scripts and Version Control", " 3.1 Scripts and Version Control Code should be maintained in script files and organized in such a way to enhance readability and comprehension. For example, use a standardized header to describe scripts and sections to make it easier to understand and find specific code of interest. Code should also be self-documenting as much as possible. Additionally, scripts should use relative filepaths for importing and exporting objects. Scripts should also be modular by focusing on one task. For example, use one script for cleaning data, another script for visualizing data, etc. A makefile can then be used to document and run all scripts in a specified order. There is an art to this organization, so just keep in mind the general principle of making code easy to understand, for your future self and for others. Scripts should be version controlled using Git, which tracks changes in code by line. Git documents changes in code when you submit a commit. Commits should be atomic by documenting single, specific changes in code as opposed to multiple, unrelated changes. Atomic commits can be small or large depending on the change being made, and they enable easier code review and make it easier to revert changes. Git commit messages should be informative and follow a certain style, such as the guide found here. Git is a command-line tool, but there are also GUI apps like GitKraken or GitFiend that may make it easier to use. RStudio also has built-in support for using Git through a GUI. There is an art to the version control process, so just keep in mind the general principle of making atomic commits. For an introduction to Git, see the Software Carpentry’s lesson at http://swcarpentry.github.io/git-novice/. emLab uses GitHub to host code in the cloud, which makes collaborative code development easier and also provides related project management tools. (You may need to be added to emLab’s GitHub organization.) Code repositories can be public or private, depending on the nature of the project. Furthermore, workflows for Git and GitHub, such as using pull requests or not, will vary from project to project. It is important that the members of each project agree to and follow a specific workflow to ensure that collaboration is effective and efficient. "],
["3-2-style-guide.html", "3.2 Style Guide", " 3.2 Style Guide At emLab, we recommend using a consistent code style for each of the different programming languages we use. Recommended coding styles for a particular language are collated in what is known as a “style guide”. Style guides typically include standardized ways of naming script files, defining functions and variables, commenting code, etc. While emLab does not mandate the use of style guides, having a consistent code style allows all emLab staff to easily understand each other’s code, collaborate on projects, and jump in on new projects. We consider this an important aspect of collaboration, reproducibility, and transparency. Rather than re-invent the wheel, we leverage existing code style guidelines for each of the languages we use. Below is a summary of the code style guidelines we recommend for various languages: R - Tidyverse Style Guide, by Hadley Wickham Python - Google’s Python Style Guide Stata - Suggestions on Stata programming style, by Nicolas J. Cox Other languages - Google style guides for other languages "],
["4-high-performance-computing.html", "4 High Performance Computing", " 4 High Performance Computing Certain use cases require high performance computing resources: big data parallel computing lengthy computation times restricted-use data For analyses involving big data or models that take a long time to estimate, a single laptop or desktop computer is often not powerful enough or becomes inconvenient to use. Additionally, for analyses involving restricted-use data, such as datasets containing personally identifiable information, data use agreements typically stipulate that the data should be stored and analyzed in a secure manner. In these cases you should use the high performance computing resources available to emLab, including cloud computing through Google Cloud Platform and the UCSB server clusters. Cloud computing incurs costs but is flexible whereas the UCSB server clusters are free but have some limitations. "],
["4-1-google-cloud-platform.html", "4.1 Google Cloud Platform", " 4.1 Google Cloud Platform "],
["4-2-ucsb-server-clusters.html", "4.2 UCSB Server Clusters", " 4.2 UCSB Server Clusters The Center for Scientific Computing at UCSB provides resources and support for research involving high performance computing, including multiple server clusters for storing and analyzing data. Find out more information at http://csc.cnsi.ucsb.edu/. For a broader introduction to high performance computing with clusters see the HPC Carpentry’s lessons at https://hpc-carpentry.github.io/. The following sections provide guidance for using the server clusters. 4.2.1 Accessing the server clusters To access the server clusters, you must first request a user account. The request form can be found at http://csc.cnsi.ucsb.edu/acct. On the form, the job type selection should probably be GPU. In the job type description you should enter any relevant information related to user groups, access to specific folders, or special software usage. If needing Stata, you should request access under the econ group, which holds the license to use Stata. The system to be used should be Pod and Knot, which are the two primary Linux-based server clusters. Pod is the newest cluster and offers greater capabilities, but Knot is still adequate for most analysis needs and typically has less active users (i.e., a shorter queue). Either cluster allows 4 TB of storage per user and more can be requested if needed. 4.2.1.1 On campus Once your account has been created, the clusters can be accessed on campus via a terminal emulator using the ssh command or via a GUI SSH app, such as X2Go. On macOS or Linux the default terminal apps can be used. On Windows the PuTTY or X2Go apps can be used. Once a terminal is started, connect to a cluster by entering one of the commands ssh user@knot.cnsi.ucsb.edu ssh user@pod.cnsi.ucsb.edu where user is your server username. You will then be prompted for your password. SSH keys can also be set up to skip password entry. Once connected, you will interact with the Linux server using the command line interface. The default working directory is your home folder /home/user. Note that this is the login node, but all analyses should be run on compute nodes. These analyses should be submitted as jobs (see below) to the queue, which allocates compute nodes. To disconnect from the server, enter exit. 4.2.1.2 Off campus The servers can be accessed off campus using the UCSB Pulse Secure Connect VPN service for secure remote access. Instructions for installing the VPN app can be found at https://www.ets.ucsb.edu/services/campus-vpn/get-connected. Once the app is installed, open it and select connect for UCSB Remote Access. You will then be prompted to enter your UCSBnetID and the associated password. Once connected, indicated by a green checkmark, you can then access the server as you normally would on campus by using a terminal or GUI SSH app. 4.2.2 Managing files File management on the server clusters involves commonly used Linux commands to navigate and modify files and folders. The generic setup is command [options] &lt;arguments&gt;. Most of the following commands have multiple options that are not covered here. For detailed documentation enter man command. Additionally, for a broader introduction to Linux commands see the Software Carpentry’s lessons at http://swcarpentry.github.io/shell-novice/. 4.2.2.1 Navigation pwd - present working directory cd dir - change directory cd - change to home directory (/home/user) cd .. - change to directory above ls - list all files in current directory ls -a - list all files, including hidden files ls -l - list all files with permission information cp source destination - copy file mv source destination - move file mkdir name - make directory mkdir -m 770 name - make directory with permissions (770 or other number) rm file - remove file rmdir dir - remove empty directory (equivalent to rm -d dir) rm -r dir - remove non-empty directory shred -uz file - securely overwrite and delete file 4.2.2.2 Viewing, creating, and editing files file file.txt - view file type information stat file.txt - view file information more file.txt - view text file one screen at a time (to exit press q) less file.txt - view text file with scrolling (to exit press q) head file.csv - print first ten rows of text file tail file.csv - print last ten rows of text file head -1 file.csv - print column names if in first row of text file cut -d , -f 2,4-6 file.csv | less - view specific columns of .csv file by number position (e.g., 2,4-6) To view .csv files with pretty output, enter cat file.csv | sed -e 's/,,/, ,/g' | column -s, -t | less -#5 -N -S Note: This may not work well for all files. Use arrow keys to navigate and enter q to exit. cat &gt; file.txt - create new text file nano - create new text file using nano text editor nano file.txt - view and edit text file using nano text editor zip -r file.zip file1 folder1 - create compressed .zip file with recursion unzip file.zip - extract .zip file into working directory 4.2.2.3 Permissions Sometimes file and folder permissions need to be modified, such as to restrict access to files. On Linux, read, write, and execute permissions are represented by octal notation and applied to the file owner, groups, and all other users. # Permission rwx 7 read, write, and execute rwx 6 read and write rw- 5 read and execute r-x 4 read only r– 3 write and execute -wx 2 write only -w- 1 execute only –x 0 none — Common permissions include -rwxrwx--- = 770 - owner and group can do everything, but others can do nothing -rwxr-x--- = 750 - owner can do everything, group can read and execute only, but others can do nothing -rwx------ = 700 - owner can do everything, but group and others can do nothing To change permissions enter chmod 770 dir - change permissions for file or directory chmod -R 770 dir - change permissions recursively for directory chgrp group file - change group ownership for file or directory 4.2.2.4 Shortcuts Ctrl-A - move to beginning of line Ctrl-E - move to end of line Ctrl-U - clear line from cursor Ctrl-C - cancel command * - wildcard completion Tab - autocompletion Up and down arrow keys - cycle through command history 4.2.2.5 Transferring files To transfer files between the server and your local computer, use scp or rsync commands in the terminal or use a SFTP GUI app. On macOS, Cyberduck and FileZilla are good options. On Windows, WinSCP and FileZilla are good options. On Linux, FileZilla is a good option. Another option is to use Globus Online, a web app for transferring large files. 4.2.3 Using rsync to transfer files The rsync command line tool provides fast, incremental file transfer. It can be used to transfer files between servers or between a local computer and a server. The primary use case for rsync is to sync two folders, such as a synced backup folder. For more detailed documentation see https://rsync.samba.org/. The generic setup for an rsync command is rsync [options] &lt;source&gt; &lt;destination&gt; With an example of transferring an entire directory from a local computer to the Pod server cluster, a typical rsync command with commonly used options is rsync -avzP -e ssh /home/user/project/ user@pod.cnsi.ucsb.edu:/home/user/project This example represents a push transfer. A pull transfer could also be completed by simply switching the source and destination. When transferring entire directories, a forward slash on the source matters but never for the destination. In this case, the ‘project’ folder has a forward slash and all its contents will be replicated exactly in the destination ‘project’ folder, copying all contents except the top ‘project’ folder. Without the forward slash on the source there would be another ‘project’ folder within the ‘project’ folder at the destination. The rsync options can be specified in short or long forms. In this use case, the -a or --archive option completely replicates all folders and files, including recursively through all subdirectories while preserving symbolic links, permissions, and ownership. The -v or --verbose option increases the amount of information that is logged. The -z or --compress option compresses files during transit to reduce transfer time. The -P or --partial --progress option enables partially transferred files to be kept in case of a break or pause and also displays the progress of individual file transfers. Many other options exist, but these are the primary ones. Additionally, add the -n or --dry-run option to test what an rsync command will do without making changes. After the command is submitted, the user will be prompted for a password if transferring to or from a server. If using rsync frequently, SSH keys can also be set up to skip password entry. For large transfers, sometimes the process needs to be paused and resumed, or sometimes the transfer can be interrupted due to a server, network, or power outage. The -P or --partial option should always be used to preserve the files or parts of files that have already been transferred and to avoid having to start over. To pause a transfer, use Ctrl-C. To resume, resubmit the same rsync command with the --append option added, which will restart the transfer where it left off. When syncing to a backup folder, add the --delete option to delete files and folders that have been deleted in the source. 4.2.4 Using software containers Software containers can be used on the server clusters via Singularity. Containers enable fully reproducible research by packaging a computing environment and the necessary software packages and applications as a self-contained image file that can be easily transferred to and run on any other system. They are also useful for collaborating across different institutions and computing infrastructures. Find out more information at http://csc.cnsi.ucsb.edu/docs/containers. 4.2.5 Submitting jobs The server clusters are shared resources, and a job queue process is used to manage and allocate resources among users. A job is simply a set of instructions that includes requests for resources and the specific set of commands to be executed, such as commands for analyzing data. When a user submits a job to the server for execution it enters the queue and is scheduled for a specific time. 4.2.5.1 Knot The Knot cluster uses TORQUE PBS to schedule jobs. First create a new .pbs file with the command nano submit.pbs The typical structure of a .pbs file for a serial job using R is #!/bin/bash #PBS -l nodes=1:ppn=12 #PBS -l walltime=2:00:00 #PBS -V cd $PBS_O_WORKDIR Rscript script.R where 1 node is requested with a 2-hour timeframe for computation. The walltime option can be excluded if the computation time is unknown. The Rscript script.R line executes the commands in the specified R script. The filepath for the script should be relative to where the .pbs file is located or an absolute path can also be used. This line would change if using different software. There are also many other #PBS options that can be included. Parallel jobs require different specifications, such as requesting more than 1 node and using mpirun commands. For another example, the structure for a serial job using Stata and that sends e-mail notifications when a job begins and ends is #!/bin/bash #PBS -l nodes=1:ppn=12 #PBS -V #PBS -m be #PBS -M user@ucsb.edu cd $PBS_O_WORKDIR/ /sw/stata/stata-mp -b do script.do To submit a job, use the command qsub submit.pbs For jobs that require less than one hour, use the short queue to minimize waiting time with the command qsub -q short submit.pbs For jobs that require large memory, use one of the commands # 256 GB/node qsub -q largemem submit.pbs # or 512 GB/node qsub -q xlargemem submit.pbs The qsub command will return a job number. To check the status of a job, use the command qstat &lt;job number&gt; To cancel or delete a job, use the command qdel &lt;job number&gt; The outputs of the analysis in the script will be returned in the same folder as the .pbs file that was submitted, typically with the filename structure submit.pbs.[job number] unless otherwise specified. 4.2.5.2 Pod The Pod cluster uses SLURM to schedule jobs. First create a new .job file with the command nano submit.job The typical structure of a .job file for a serial job using R is #!/bin/bash -l #Serial #SBATCH --nodes=1 --ntasks-per-node=1 cd $SLURM_SUBMIT_DIR module load R Rscript script.R where 1 node is requested. The module load R line loads R, and then the Rscript script.R line executes the commands in the specified R script. The filepath for the script should be relative to where the .job file is located or an absolute path can also be used. These last two lines would change if using different software. There are also many other #SBATCH options that can be included. Parallel jobs require different specifications, such as requesting more than 1 node and using mpirun commands. For another example, the structure for a serial job using Stata and that sends e-mail notifications is #!/bin/bash -l #Serial #SBATCH --nodes=1 --ntasks-per-node=1 #SBATCH --mail-type=ALL #SBATCH --mail-user=user@ucsb.edu cd $SLURM_SUBMIT_DIR /sw/stata/stata-mp -b do script.do To submit a job, use the command sbatch submit.job For jobs that require less than one hour, use the short queue to minimize waiting time with the command sbatch -p short submit.job For jobs that require large memory, use the command # 1.5 TB/node sbatch -p fat submit.job The sbatch command will return a job number. To check the status of a job, use one of the commands showq &lt;job number&gt; # or showq | grep mdono # or squeue -u $USER To cancel or delete a job, use the command scancel -i &lt;job number&gt; The outputs of the analysis in the script will be returned in the same folder as the .job file that was submitted, typically with the filename structure slurm-[jobnumber].out unless otherwise specified. 4.2.5.3 Available software The software available on the clusters includes R, Stata, Python, Julia, MATLAB, Git, and LaTeX. Additional information about available software, using different versions of software, and installing packages can be found at http://csc.cnsi.ucsb.edu/docs/scientific-software. "]
]
